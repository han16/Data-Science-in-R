---
title: "lecture13"
author: "Shengtong"
date: "2018-11-15"
output: workflowr::wflow_html
---


```{r, echo=T, eval=F}
########################################################
#### Project presentations
####
#### Final take-home exam next week
######################################################

#=============================================================
## Regular expressions (Regex)
# Extra resource: https://regexr.com/
#=============================================================

## Character data can be hard to deal with!
## Regular expressions = way to express patterns to match in text or 
##   search and replace
##
## Basic tasks:
## (1) Study a single character vector 
##  - How long are strings?
##  - Presence/absence of literal string
## (2) Operate on single character vector
##  - Keep/discard elements that contain a string
##  - Split into two or more vectors using a delimiter
##  - Snip out pieces of strings based on position
## (3) Operate on two or more character vectors
##  - Glue them together element-wise

#-----------------------------------------------------------------
## First: let's look at regex-free string manipulation in R
#-----------------------------------------------------------------

## We will use the stringr package (part of the tidyverse)
##   Main functions start with str_*
## Note: base functions include nchar(), strsplit(), substr(), grep(), ...

library(tidyverse)

## fruit, words, sentences
## => Which fruits actually contain the word "fruit"?
str_detect(fruit, pattern = "fruit")
str_subset(fruit, pattern = "fruit")
str_subset(fruit, pattern = "Fruit")  ## Note: case sensitive
my_fruit <- str_subset(fruit, pattern = "fruit")

## Split on a delimiter
str_split(my_fruit, pattern = " ") ## Why is this a list?
str_split_fixed(my_fruit, pattern = " ", n = 2)
## Remember, we already knew how to do this with tidyr commands:
my_fruit_df <- data.frame(my_fruit)
my_fruit_df %>% 
  separate(my_fruit, into = c("pre", "post"), sep = " ")

## Substring extraction and replacement
length(my_fruit)
str_length(my_fruit)
head(fruit) %>% str_sub(1, 3)
tibble(fruit) %>% 
  head() %>% 
  mutate(snip = str_sub(fruit, 1:6, 3:8))  ## This can be vectorized
x <- head(fruit, 3)
str_sub(x, 1, 3) <- "AAA"  ## Can also be used for assignment
x

## Collapse a vector to a single string
head(fruit) %>% 
  str_c(collapse = ", ")

## Or create a new vector by collapsing element-wise 
## => we can also do this with tidyr::unite()
str_c(fruit[1:4], fruit[5:8], sep = " & ")

## Replace a substring
str_replace(my_fruit, pattern = "fruit", replacement = "THINGY")
## Special case is str_replace_na
x[1] <- NA
str_replace_na(x, "UNKNOWN!")

#-----------------------------------------------------------------
## Now: regular expressions in R
#-----------------------------------------------------------------

## Let's use the gapminder data for this part

library(gapminder)
countries <- levels(gapminder$country)

## Frequently you can't do your character task with a fixed string but 
##  need to use a pattern instead => regex is standard way to deal with this!

## Use metacharacters:
## . = any standard character except a newline
## \n = newline
## \d = [:digit:] = digit
## [:lower:], [:upper:]: lowercase and uppercase

str_subset(countries, pattern = "i.a")  ## Note: case sensitive! Italy doesn't match...
str_subset(countries, pattern = regex("i.a"))
str_subset(countries, pattern = "I.a")

## Anchors specify where an expression must occur within a string
## ^ = start
## $ = end

str_subset(countries, pattern = "i.a$")
str_subset(my_fruit, pattern = "d")
str_subset(my_fruit, pattern = "^d")

## Some metacharacters need "escaping" = an additional backslash
## This is the case for any characters that would otherwise have a special interpretation
## e.g. $ * + . ? [ ] ^ { } | ( ) \
## \\b = word boundary
## \\B = NOT a word boundary

str_subset(fruit, pattern = "melon")
str_subset(fruit, pattern = "bmelon")
str_subset(fruit, pattern = "\\Bmelon")
str_subset(countries, pattern = ".") ## This doesn't work because . is a special character!
str_subset(countries, pattern = "\\.") ## This doesn't work because . is a special character!
x <- c("whatever", "X is distributed U[0,1]")
str_subset(x, pattern = "\\[")

## | = or
str_subset(fruit, pattern = "apple|berry")

## Character classes are indicated by square brackets
## [...] = one of the characters in this class
## [^...] = NOT one of the characters in this class
str_subset(countries, pattern = "[nls]ia$")
str_subset(countries, pattern = "[^nls]ia$")

## \s = space
## [[:space:]] = space
## [[:punct:]] = punctuation
str_split_fixed(my_fruit, pattern = " ", n = 2)
str_split_fixed(my_fruit, pattern = "\\s", n = 2)
str_split_fixed(my_fruit, pattern = "[[:space:]]", n = 2)
str_subset(countries, "[[:punct:]]")

## Quantifiers:
## * = 0 or more
## + = 1 or more
## ? = 0 or 1
## {n} = exactly n
## {n,} = at least n
## {,m} = at most m
## {n,m} = between n and m, inclusive

## Let's play around with this: how does changing the quantifier change the result?
str_subset(fruit, pattern = "l[^\\s]{2}e")

tmp <- c("file_record_transcript.pdf",
         "file_07241999.pdf",
         "testfile_fake.pdf.tmptest")
str_subset(tmp, pattern = "^(file.*)\\.pdf$")
str_subset(tmp, pattern = "^file.*[^.pdf]")


#-----------------------------------------------------------------
## In class: https://regexone.com/lesson/introduction_abcs
##
## Play around with the Million Song Database
## -- Find all song titles with love/loving/lovin'.
## -- Find all artists with punctuation in their names.
#-----------------------------------------------------------------

library(devtools)
install_github("JoeyBernhardt/singer")
library(singer) ## Data package with an excerpt from the Million Song Database
places <- unique(singer_locations$city)
song_titles <- singer_locations$title


#=============================================================
## Sentiment analysis aka opinion mining
## Read this: http://varianceexplained.org/r/trump-tweets/
## And this: http://varianceexplained.org/r/trump-followup/
##
## https://www.tidytextmining.com/
#=============================================================

## Aim: determine the attitude of someone/thing wrt some topic/context/documents/etc
##   (often used by companies to quantify general social media opinion)
## Idea: attribute to each word a score, expressing whether its negative or positive,
##   and sum it up
## Classify polarity (= positive, neutral, negative) or beyond polarity 
##   (angry, sad, happy, ...)


## 1. obtain your text source (website, Twitter, database, PDF document, ...)
## 2. extract documents and move into a corpus
## 3. transformation (convert to lowercase, remove punctuation, remove stopwords, ...)
## 4. extract features
## 5. perform analysis (word frequency, co-occurrrence, document classification/comparison,
##      topic modeling)

#-----------------------------------------------------------------
## Polarity analysis
#-----------------------------------------------------------------

## Now let's look at the tidy data frame in austen_books()
## -- We will first add a linenumber variable to keep track of original line numbers.
## -- Then let's use a regex to find where the chapters are.
## -- Then we must break text up into words using unnest_tokens, 
##  remove those not considered interesting (="stop words"),
##  join text data frame to the vocabulary scores, and do the math!

library(tidytext)
library(janeaustenr)

original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, 
                                     regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%
  ungroup()
original_books

## We then use the unnest_tokens function to return text in one-token-per-row tidy format
tidy_books <- original_books %>%
  unnest_tokens(word, text)

## Now we can use tidyverse data cleaning to remove the "stop words"
data(stop_words)
head(stop_words)
tidy_books <- tidy_books %>%
  anti_join(stop_words)

## And dplyr can tell us about e.g. the most common words in all of Jane Austen's novels
tidy_books %>%
  group_by(book) %>%
  count(word) %>%
  top_n(1)

## Now need a vocabulary of words for which we have the scores
## Choices: afinn, bing, nrc, loughran
bing <- get_sentiments("bing")
get_sentiments("afinn")
get_sentiments("nrc")


## Now we combine the vocabulary with our tidy text, and count up for pre-defined sections
janeaustensentiment <- tidy_books %>%
  inner_join(bing) %>% 
  count(book, index = linenumber %/% 80, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)

janeaustensentiment

## And let's plot this out!
ggplot(janeaustensentiment, aes(index, sentiment, fill = book)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x") +
  theme_minimal(base_size = 13) +
  labs(title = "Sentiment in Jane Austen's Novels",
       y = "Sentiment") +
  scale_x_discrete(expand=c(0.02,0)) +
  theme(strip.text=element_text(hjust=0)) +
  theme(strip.text = element_text(face = "italic")) +
  theme(axis.title.x=element_blank()) +
  theme(axis.ticks.x=element_blank()) +
  theme(axis.text.x=element_blank())

## Let's ask another question: What is the most negative chapter of each
##   Jane Austen novel?
## 1. Identify "negative" Bing words
## 2. Group words by chapter, and count the total number of words per chapter
##   (to normalize for differences in length)

bingnegative <- sentiments %>%
  filter(lexicon == "bing", sentiment == "negative")

wordcounts <- tidy_books %>%
  group_by(book, chapter) %>%
  summarize(words = n())

tidy_books %>%
  semi_join(bingnegative) %>%
  group_by(book, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("book", "chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  filter(chapter != 0) %>%
  top_n(1)

## And what about looking at words that co-occur together? => Use pair_count
install_github("dgrtwo/widyr")
library(widyr)
pride_prejudice_words <- tidy_books %>%
  filter(book == "Pride & Prejudice")
word_cooccurences <- pride_prejudice_words %>%
  pairwise_count(word, linenumber, sort = TRUE, upper=FALSE)
word_cooccurences

## Now we could plot a network of co-occurring words
library(igraph)
library(ggraph)

set.seed(1813)
word_cooccurences %>%
  filter(n >= 10) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
  geom_node_point(color = "darkslategray4", size = 5) +
  geom_node_text(aes(label = name), vjust = 1.8) +
  ggtitle(expression(paste("Word Network in Jane Austen's ", 
                           italic("Pride and Prejudice")))) +
  theme_void()

## And another book
pride_prejudice_words <- tidy_books %>%
  filter(book == "Emma")
word_cooccurences <- pride_prejudice_words %>%
  pair_count(linenumber, word, sort = TRUE)
set.seed(2016)
word_cooccurences %>%
  filter(n >= 10) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
  geom_node_point(color = "plum4", size = 5) +
  geom_node_text(aes(label = name), vjust = 1.8) +
  ggtitle(expression(paste("Word Network in Jane Austen's ", 
                           italic("Emma")))) +
  theme_void()

## Word clouds are a common tool too
library(wordcloud)
tb <- tidy_books %>%
  count(word) 
wordcloud(tb$word, tb$n, max.words = 100)

tb <- tidy_books %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE)
tb_wide <- spread(tb, sentiment, n)
tb_wide[is.na(tb_wide)] <- 0
tb_wide_mat <- as.matrix(tb_wide[,2:3])
rownames(tb_wide_mat) <- unlist(tb_wide[,1])
comparison.cloud(tb_wide_mat, colors = c("#F8766D", "#00BFC4"),
                max.words = 100)

#-----------------------------------------------------------------
## Beyond polarity
## Topic modeling:  
## Uses Natural Language Processing techniques to automate analysis on 
##   large collections of texts => probabilistic topic models that use
##   Latent Dirichlet Allocation (LDA)
#-----------------------------------------------------------------

## Sometimes tokenizing at the word level does not make sense:
## e.g., I am not having a good day.
## In this case, we probably want to tokenize by sentences rather than words.

austen_sentences <- austen_books() %>% 
  group_by(book) %>% 
  unnest_tokens(sentence, text, token = "sentences") %>% 
  ungroup()
austen_sentences$sentence[39]


#-----------------------------------------------------------------
## In class:
## Let's play around with text from a data package, this time using
##   the "nrc" lexicon
#-----------------------------------------------------------------
library(gutenbergr)
gutenberg_metadata
hgwells <- gutenberg_download(c(35, 36, 5230, 159))

devtools::install_github("bradleyboehmke/harrypotter")
library(harrypotter)
  



#=============================================================
## Next week: class cancelled, final take-home exam!
#=============================================================

## Potential topics for final exam:
##
## -- for/while loops, Boolean logic, Monte Carlo simulation
## -- functions
## -- error debugging based on an error/warning message
## -- effective plotting with ggplot2
## -- data wrangling with the tidyverse and dealing with messy data
## -- linear/logistic regression (coefficient interpretation, model diagnostics, etc)
## -- model selection 
## -- multiple testing correction for p-values
## -- bootstrapping and permutation tests
## -- basics of Shiny programming
## -- K-means clustering, hierarchical clustering, principal components analysis
## -- text/sentiment analysis
## -- confounding/Simpson's paradox


```
