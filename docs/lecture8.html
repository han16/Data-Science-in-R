<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Shengtong" />

<meta name="date" content="2018-11-06" />

<title>lecture8</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.0.13/css/fa-svg-with-js.css" rel="stylesheet" />
<script src="site_libs/font-awesome-5.0.13/js/fontawesome-all.min.js"></script>
<script src="site_libs/font-awesome-5.0.13/js/fa-v4-shims.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Code</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="license.html">License</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://han16.github.io/Data-Science-in-R/">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<!-- Add a small amount of space between sections. -->
<style type="text/css">
div.section {
  padding-top: 12px;
}
</style>

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">lecture8</h1>
<h4 class="author"><em>Shengtong</em></h4>
<h4 class="date"><em>2018-11-06</em></h4>

</div>


<p><strong>Last updated:</strong> 2018-11-07</p>
<strong>workflowr checks:</strong> <small>(Click a bullet for more information)</small>
<ul>
<li>
<p><details> <summary> <strong style="color:red;">✖</strong> <strong>R Markdown file:</strong> uncommitted changes </summary> The R Markdown file has unstaged changes. To know which version of the R Markdown file created these results, you’ll want to first commit it to the Git repo. If you’re still working on the analysis, you can ignore this warning. When you’re finished, you can run <code>wflow_publish</code> to commit the R Markdown file and build the HTML.</p>
</details>
</li>
<li>
<p><details> <summary> <strong style="color:blue;">✔</strong> <strong>Environment:</strong> empty </summary></p>
<p>Great job! The global environment was empty. Objects defined in the global environment can affect the analysis in your R Markdown file in unknown ways. For reproduciblity it’s best to always run the code in an empty environment.</p>
</details>
</li>
<li>
<p><details> <summary> <strong style="color:blue;">✔</strong> <strong>Seed:</strong> <code>set.seed(20181026)</code> </summary></p>
<p>The command <code>set.seed(20181026)</code> was run prior to running the code in the R Markdown file. Setting a seed ensures that any results that rely on randomness, e.g. subsampling or permutations, are reproducible.</p>
</details>
</li>
<li>
<p><details> <summary> <strong style="color:blue;">✔</strong> <strong>Session information:</strong> recorded </summary></p>
<p>Great job! Recording the operating system, R version, and package versions is critical for reproducibility.</p>
</details>
</li>
<li>
<p><details> <summary> <strong style="color:blue;">✔</strong> <strong>Repository version:</strong> <a href="https://github.com/han16/Data-Science-in-R/tree/b63bec6f1dd61dfe8567d640c2e81453eb099d79" target="_blank">b63bec6</a> </summary></p>
Great! You are using Git for version control. Tracking code development and connecting the code version to the results is critical for reproducibility. The version displayed above was the version of the Git repository at the time these results were generated. <br><br> Note that you need to be careful to ensure that all relevant files for the analysis have been committed to Git prior to generating the results (you can use <code>wflow_publish</code> or <code>wflow_git_commit</code>). workflowr only checks the R Markdown file, but you know if there are other scripts or data files that it depends on. Below is the status of the Git repository when the results were generated:
<pre><code>
Unstaged changes:
    Modified:   analysis/index.Rmd
    Modified:   analysis/lecture8.Rmd

</code></pre>
Note that any generated files, e.g. HTML, png, CSS, etc., are not included in this status report because it is ok for generated content to have uncommitted changes. </details>
</li>
</ul>
<details> <summary> <small><strong>Expand here to see past versions:</strong></small> </summary>
<ul>
<table style="border-collapse:separate; border-spacing:5px;">
<thead>
<tr>
<th style="text-align:left;">
File
</th>
<th style="text-align:left;">
Version
</th>
<th style="text-align:left;">
Author
</th>
<th style="text-align:left;">
Date
</th>
<th style="text-align:left;">
Message
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Rmd
</td>
<td style="text-align:left;">
<a href="https://github.com/han16/Data-Science-in-R/blob/b63bec6f1dd61dfe8567d640c2e81453eb099d79/analysis/lecture8.Rmd" target="_blank">b63bec6</a>
</td>
<td style="text-align:left;">
han16
</td>
<td style="text-align:left;">
2018-11-07
</td>
<td style="text-align:left;">
Nov72018
</td>
</tr>
<tr>
<td style="text-align:left;">
Rmd
</td>
<td style="text-align:left;">
<a href="https://github.com/han16/Data-Science-in-R/blob/e0d147d5d52571c897621f3f5d0af5da8e546e8d/analysis/lecture8.Rmd" target="_blank">e0d147d</a>
</td>
<td style="text-align:left;">
han16
</td>
<td style="text-align:left;">
2018-11-07
</td>
<td style="text-align:left;">
Nov72018
</td>
</tr>
</tbody>
</table>
</ul>
<p></details></p>
<hr />
<pre class="r"><code>########################################################
#### Go over midterm
####
#### Read Chapter 23 from http://r4ds.had.co.nz/index.html,
####   Chapter 14 from book
####
#### HW 5 due next week
####
#### Review correlation, linear models
######################################################


#-------------------------------------------------------------
## In class warm-up to review linear models:
#-------------------------------------------------------------
## What is linear regression?
## How are coefficients of a linear regression interpreted?
## Y = B_0 + B_1 * X
## What is the idea behind ordinary least squares regression?

library(tidyverse)</code></pre>
<pre><code>-- Attaching packages ----------------------------------------------------------------------------- tidyverse 1.2.1 --</code></pre>
<pre><code>v ggplot2 3.1.0     v purrr   0.2.5
v tibble  1.4.2     v dplyr   0.7.7
v tidyr   0.8.2     v stringr 1.3.1
v readr   1.1.1     v forcats 0.3.0</code></pre>
<pre><code>-- Conflicts -------------------------------------------------------------------------------- tidyverse_conflicts() --
x dplyr::filter() masks stats::filter()
x dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(broom)
data(iris)
setosa &lt;- iris %&gt;% filter(Species == &quot;setosa&quot;)
ggplot(setosa, aes(x=Sepal.Width, y=Sepal.Length)) +
  geom_point() +
  geom_smooth(method=&quot;lm&quot;, se=FALSE)</code></pre>
<p><img src="figure/lecture8.Rmd/unnamed-chunk-1-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>mod  &lt;- lm(Sepal.Length ~ Sepal.Width, 
           data = setosa)
summary(mod)</code></pre>
<pre><code>
Call:
lm(formula = Sepal.Length ~ Sepal.Width, data = setosa)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.52476 -0.16286  0.02166  0.13833  0.44428 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   2.6390     0.3100   8.513 3.74e-11 ***
Sepal.Width   0.6905     0.0899   7.681 6.71e-10 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.2385 on 48 degrees of freedom
Multiple R-squared:  0.5514,    Adjusted R-squared:  0.542 
F-statistic: 58.99 on 1 and 48 DF,  p-value: 6.71e-10</code></pre>
<pre class="r"><code>mod2  &lt;- lm(Sepal.Length ~ Sepal.Width + Species, 
           data = iris)
summary(mod2)</code></pre>
<pre><code>
Call:
lm(formula = Sepal.Length ~ Sepal.Width + Species, data = iris)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.30711 -0.25713 -0.05325  0.19542  1.41253 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         2.2514     0.3698   6.089 9.57e-09 ***
Sepal.Width         0.8036     0.1063   7.557 4.19e-12 ***
Speciesversicolor   1.4587     0.1121  13.012  &lt; 2e-16 ***
Speciesvirginica    1.9468     0.1000  19.465  &lt; 2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.438 on 146 degrees of freedom
Multiple R-squared:  0.7259,    Adjusted R-squared:  0.7203 
F-statistic: 128.9 on 3 and 146 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>#=============================================================
## Linear models: lm(Numeric response ~ &lt;predictors&gt;)
## 
## Single numeric predictor = Regression
## Single categorical predictor = ANOVA
## Multiple numeric predictors = Multiple regression
## Multiple categorical predictors = n-way ANOVA
## Single categorical and n numeric predictors = ANCOVA
## Multiple categorical and n numeric predictors = linear model
#=============================================================

## What happens with a categorical predictor?
mod_species &lt;- lm(Sepal.Length ~ Species, data=iris)
summary(mod_species) </code></pre>
<pre><code>
Call:
lm(formula = Sepal.Length ~ Species, data = iris)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.6880 -0.3285 -0.0060  0.3120  1.3120 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         5.0060     0.0728  68.762  &lt; 2e-16 ***
Speciesversicolor   0.9300     0.1030   9.033 8.77e-16 ***
Speciesvirginica    1.5820     0.1030  15.366  &lt; 2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.5148 on 147 degrees of freedom
Multiple R-squared:  0.6187,    Adjusted R-squared:  0.6135 
F-statistic: 119.3 on 2 and 147 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>## Where is the Speciessetosa coefficient?
## How are these coefficients interpreted?</code></pre>
<pre class="r"><code>augment_species &lt;- augment(mod_species)
ggplot(augment_species, aes(x=Species, y=Sepal.Length)) +
  geom_point() +
  geom_point(aes(x=Species, y=.fitted), color=&quot;red&quot;, size=4)</code></pre>
<p><img src="figure/lecture8.Rmd/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>## How to interpret multiple numeric predictors? 
mod_multiple  &lt;- lm(Sepal.Length ~ Sepal.Width + Petal.Width, 
           data = setosa)
summary(mod_multiple)</code></pre>
<pre><code>
Call:
lm(formula = Sepal.Length ~ Sepal.Width + Petal.Width, data = setosa)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.50350 -0.17022  0.02213  0.15569  0.46314 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.63000    0.30928   8.504 4.56e-11 ***
Sepal.Width  0.66640    0.09219   7.229 3.68e-09 ***
Petal.Width  0.37227    0.33159   1.123    0.267    
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.2379 on 47 degrees of freedom
Multiple R-squared:  0.5631,    Adjusted R-squared:  0.5445 
F-statistic: 30.29 on 2 and 47 DF,  p-value: 3.541e-09</code></pre>
<pre class="r"><code>## How to interpret multiple numeric/categorical predictors? 
mod_multiple  &lt;- lm(Sepal.Length ~ Sepal.Width + Species, 
                    data = iris)
summary(mod_multiple)</code></pre>
<pre><code>
Call:
lm(formula = Sepal.Length ~ Sepal.Width + Species, data = iris)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.30711 -0.25713 -0.05325  0.19542  1.41253 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         2.2514     0.3698   6.089 9.57e-09 ***
Sepal.Width         0.8036     0.1063   7.557 4.19e-12 ***
Speciesversicolor   1.4587     0.1121  13.012  &lt; 2e-16 ***
Speciesvirginica    1.9468     0.1000  19.465  &lt; 2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.438 on 146 degrees of freedom
Multiple R-squared:  0.7259,    Adjusted R-squared:  0.7203 
F-statistic: 128.9 on 3 and 146 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>## Reminder: we can always check the model matrix to see the equation fit by lm!
library(modelr)</code></pre>
<pre><code>
Attaching package: &#39;modelr&#39;</code></pre>
<pre><code>The following object is masked from &#39;package:broom&#39;:

    bootstrap</code></pre>
<pre class="r"><code>model_matrix(iris, Sepal.Length ~ Sepal.Width + Species)</code></pre>
<pre><code># A tibble: 150 x 4
   `(Intercept)` Sepal.Width Speciesversicolor Speciesvirginica
           &lt;dbl&gt;       &lt;dbl&gt;             &lt;dbl&gt;            &lt;dbl&gt;
 1             1         3.5                 0                0
 2             1         3                   0                0
 3             1         3.2                 0                0
 4             1         3.1                 0                0
 5             1         3.6                 0                0
 6             1         3.9                 0                0
 7             1         3.4                 0                0
 8             1         3.4                 0                0
 9             1         2.9                 0                0
10             1         3.1                 0                0
# ... with 140 more rows</code></pre>
<pre class="r"><code>## IMPORTANT: Predictors are assumed to be independent of one another 
## in a multiple regression!!
## Don&#39;t forget to check model assumptions!</code></pre>
<div id="interactions" class="section level2">
<h2>Interactions</h2>
<pre class="r"><code>library(GGally)</code></pre>
<pre><code>
Attaching package: &#39;GGally&#39;</code></pre>
<pre><code>The following object is masked from &#39;package:dplyr&#39;:

    nasa</code></pre>
<pre class="r"><code>ggpairs(iris)</code></pre>
<pre><code>`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<pre><code>`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="figure/lecture8.Rmd/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#=============================================================
## Interactions
#=============================================================

## What is an interaction?

## What if the relationship between Sepal.Length and Sepal.Width is different
##    depending on the species?
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) +
  geom_point(aes(color=Species)) +
  geom_smooth(se=FALSE, aes(color=Species), method = &quot;lm&quot;)</code></pre>
<p><img src="figure/lecture8.Rmd/unnamed-chunk-7-2.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>mod_interaction &lt;- lm(Sepal.Length ~ Sepal.Width * Species,
                       data = iris)
summary(mod_interaction)</code></pre>
<pre><code>
Call:
lm(formula = Sepal.Length ~ Sepal.Width * Species, data = iris)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.26067 -0.25861 -0.03305  0.18929  1.44917 

Coefficients:
                              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                     2.6390     0.5715   4.618 8.53e-06 ***
Sepal.Width                     0.6905     0.1657   4.166 5.31e-05 ***
Speciesversicolor               0.9007     0.7988   1.128    0.261    
Speciesvirginica                1.2678     0.8162   1.553    0.123    
Sepal.Width:Speciesversicolor   0.1746     0.2599   0.672    0.503    
Sepal.Width:Speciesvirginica    0.2110     0.2558   0.825    0.411    
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.4397 on 144 degrees of freedom
Multiple R-squared:  0.7274,    Adjusted R-squared:  0.718 
F-statistic: 76.87 on 5 and 144 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>## Interactions: continuous and categorical predictors
mod1a &lt;- lm(Sepal.Width ~ Sepal.Length + Species, data=iris)
mod1b &lt;- lm(Sepal.Width ~ Sepal.Length * Species, data=iris)
mod1c &lt;- lm(Sepal.Width ~ Sepal.Length + Species + Sepal.Length:Species, 
            data=iris)</code></pre>
<pre class="r"><code>## Use commands from modelr to make predictions/calculate residuals
library(modelr)
## Let&#39;s visualize residuals and evaluate models
res &lt;- iris %&gt;% gather_residuals(mod1a, mod1b)
ggplot(res, aes(x=Sepal.Length, y=resid, colour = Species)) + 
  geom_point() + 
  facet_grid(model ~ Species)</code></pre>
<p><img src="figure/lecture8.Rmd/unnamed-chunk-9-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>grid &lt;- data_grid(iris, Sepal.Length, Sepal.Width, Species) %&gt;%
  gather_predictions(mod1a, mod1b)
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, colour = Species)) + 
  geom_point() + 
  geom_line(data = grid, aes(y = pred)) + 
  facet_wrap(~ model) + theme_bw()</code></pre>
<p><img src="figure/lecture8.Rmd/unnamed-chunk-9-2.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>## Interactions: continuous and continuous predictors
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) +
  geom_point(aes(color=Petal.Width))</code></pre>
<p><img src="figure/lecture8.Rmd/unnamed-chunk-10-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>mod2a &lt;- lm(Sepal.Width ~ Sepal.Length + Petal.Width, data=iris)
mod2b &lt;- lm(Sepal.Width ~ Sepal.Length * Petal.Width, data=iris)

grid &lt;- data_grid(iris, Sepal.Length, Petal.Width, Species) %&gt;%
  gather_predictions(mod2a, mod2b)
grid$Petal.Width.Quantile &lt;- factor(ntile(grid$Petal.Width, 4))
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) + 
  geom_point(aes(colour = Species)) + 
  geom_smooth(data = grid, alpha=0.5, color=&quot;black&quot;,
              aes(y = pred, lty=Petal.Width.Quantile)) + 
  facet_wrap(~ model) + theme_bw() </code></pre>
<pre><code>`geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="figure/lecture8.Rmd/unnamed-chunk-10-2.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>## Typically, main effects are always included with interaction effects
## (but ignore main effect coefficients when interaction is significant)</code></pre>
</div>
<div id="logistic-regression" class="section level2">
<h2>Logistic regression</h2>
<pre class="r"><code>#=============================================================
## Logistic regression
#=============================================================

## Big question:
## Why did some people survive the sinking of the Titanic while others 
## did not?
##
## Survived is the response variable, remaining variables are predictors

library(titanic)

## First idea could be to fit a linear model to predict survival based on age

ggplot(titanic_train, aes(Age, Survived)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<pre><code>Warning: Removed 177 rows containing non-finite values (stat_smooth).</code></pre>
<pre><code>Warning: Removed 177 rows containing missing values (geom_point).</code></pre>
<p><img src="figure/lecture8.Rmd/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>## What do you notice?</code></pre>
<pre class="r"><code>ggplot(titanic_train, aes(Age, Survived)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) +
  xlim(0, 200)</code></pre>
<pre><code>Warning: Removed 177 rows containing non-finite values (stat_smooth).</code></pre>
<pre><code>Warning: Removed 177 rows containing missing values (geom_point).</code></pre>
<p><img src="figure/lecture8.Rmd/unnamed-chunk-12-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>## =&gt; So rather than modeling Survived directly, we could instead model
## the probability that Survived == 1, given the age:
## P = Pr(Survived = 1 | age)
## i.e., model a binary response by fitting a logistic curve to data
## We could say P &gt; 0.5 =&gt; survived, P &lt;= 0.5 =&gt; did not survive
##</code></pre>
<pre class="r"><code>## This is done using a logistic regression:
survive_age &lt;- glm(Survived ~ Age, data = titanic_train, family = binomial)
summary(survive_age)</code></pre>
<pre><code>
Call:
glm(formula = Survived ~ Age, family = binomial, data = titanic_train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.1488  -1.0361  -0.9544   1.3159   1.5908  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept) -0.05672    0.17358  -0.327   0.7438  
Age         -0.01096    0.00533  -2.057   0.0397 *
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 964.52  on 713  degrees of freedom
Residual deviance: 960.23  on 712  degrees of freedom
  (177 observations deleted due to missingness)
AIC: 964.23

Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code>ggplot(titanic_train, aes(Age, Survived)) +
  geom_point() +
  geom_smooth(method = &quot;glm&quot;,
              method.args = list(family = &quot;binomial&quot;),
              se = FALSE)</code></pre>
<pre><code>Warning: Removed 177 rows containing non-finite values (stat_smooth).</code></pre>
<pre><code>Warning: Removed 177 rows containing missing values (geom_point).</code></pre>
<p><img src="figure/lecture8.Rmd/unnamed-chunk-13-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>## Note that the line is not actually perfectly linear!
## And no more negative predictions of survival probability
ggplot(titanic_train, aes(Age, Survived)) +
  geom_point() +
  geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;binomial&quot;),
              se = FALSE, fullrange = TRUE) +
  xlim(0, 200)</code></pre>
<pre><code>Warning: Removed 177 rows containing non-finite values (stat_smooth).</code></pre>
<pre><code>Warning: Removed 177 rows containing missing values (geom_point).</code></pre>
<p><img src="figure/lecture8.Rmd/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>## Interpretation of coefficients
## For every unit increase of age, the log odds of survival increase
##    by the coefficient
## log odds = log((Pr survived) / (Pr did not survive))

## Adding predictions using modelr
## Make a clean grid of potential values
titanic_age &lt;- titanic_train %&gt;%
  data_grid(Age)
titanic_age</code></pre>
<pre><code># A tibble: 89 x 1
     Age
   &lt;dbl&gt;
 1  0.42
 2  0.67
 3  0.75
 4  0.83
 5  0.92
 6  1   
 7  2   
 8  3   
 9  4   
10  5   
# ... with 79 more rows</code></pre>
<pre class="r"><code>titanic_age &lt;- titanic_age %&gt;%
  add_predictions(survive_age)</code></pre>
<pre class="r"><code>## So what&#39;s going on with these predictions?
head(titanic_age)</code></pre>
<pre><code># A tibble: 6 x 2
    Age    pred
  &lt;dbl&gt;   &lt;dbl&gt;
1  0.42 -0.0613
2  0.67 -0.0641
3  0.75 -0.0649
4  0.83 -0.0658
5  0.92 -0.0668
6  1    -0.0677</code></pre>
<pre class="r"><code>#-------------------------------------------------------------
## In-class exercises:
##
## Write a function called logit2prob to convert log-odds to 
## the predicted scale
#-------------------------------------------------------------
logit2prob &lt;- function(x) {
  return(exp(x) / (1 + exp(x)))
}


titanic_age &lt;- titanic_age %&gt;%
  mutate(pred = logit2prob(pred))

ggplot(titanic_age, aes(Age, pred)) +
  geom_line() +
  labs(title = &quot;Relationship Between Age and Surviving the Titanic&quot;,
       y = &quot;Predicted Probability of Survival&quot;)</code></pre>
<pre><code>Warning: Removed 1 rows containing missing values (geom_path).</code></pre>
<p><img src="figure/lecture8.Rmd/unnamed-chunk-17-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>## But maybe two (or more!) predictors are important!
survive_age.sex &lt;- glm(formula = Survived ~ Age + Sex, 
                       family = binomial, 
                       data = titanic_train)
summary(survive_age.sex)</code></pre>
<pre><code>
Call:
glm(formula = Survived ~ Age + Sex, family = binomial, data = titanic_train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7405  -0.6885  -0.6558   0.7533   1.8989  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.277273   0.230169   5.549 2.87e-08 ***
Age         -0.005426   0.006310  -0.860     0.39    
Sexmale     -2.465920   0.185384 -13.302  &lt; 2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 964.52  on 713  degrees of freedom
Residual deviance: 749.96  on 711  degrees of freedom
  (177 observations deleted due to missingness)
AIC: 755.96

Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code>## What do the coefficients in the model mean here?
titanic_age.sex &lt;- titanic_train %&gt;%
  data_grid(Age, Sex) %&gt;%
  add_predictions(survive_age.sex) %&gt;%
  mutate(pred = logit2prob(pred))
titanic_age.sex</code></pre>
<pre><code># A tibble: 178 x 3
     Age Sex     pred
   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;
 1  0.42 female 0.782
 2  0.42 male   0.233
 3  0.67 female 0.781
 4  0.67 male   0.233
 5  0.75 female 0.781
 6  0.75 male   0.233
 7  0.83 female 0.781
 8  0.83 male   0.233
 9  0.92 female 0.781
10  0.92 male   0.233
# ... with 168 more rows</code></pre>
<pre class="r"><code>## So how to interpret this?
ggplot(titanic_age.sex, aes(Age, pred, color = Sex)) +
  geom_line() +
  labs(title = &quot;Probability of Surviving the Titanic&quot;,
       y = &quot;Predicted Probability of Survival&quot;,
       color = &quot;Sex&quot;)</code></pre>
<pre><code>Warning: Removed 2 rows containing missing values (geom_path).</code></pre>
<p><img src="figure/lecture8.Rmd/unnamed-chunk-20-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>## So what about a possible interaction?

survive_ageXsex &lt;- glm(formula = Survived ~ Age * Sex, family = binomial, 
                       data = titanic_train)
summary(survive_ageXsex)</code></pre>
<pre><code>
Call:
glm(formula = Survived ~ Age * Sex, family = binomial, data = titanic_train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.9401  -0.7136  -0.5883   0.7626   2.2455  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  0.59380    0.31032   1.913  0.05569 . 
Age          0.01970    0.01057   1.863  0.06240 . 
Sexmale     -1.31775    0.40842  -3.226  0.00125 **
Age:Sexmale -0.04112    0.01355  -3.034  0.00241 **
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 964.52  on 713  degrees of freedom
Residual deviance: 740.40  on 710  degrees of freedom
  (177 observations deleted due to missingness)
AIC: 748.4

Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code>## How are these coefficients interpreted?
titanic_ageXsex &lt;- titanic_train %&gt;%
  data_grid(Age, Sex) %&gt;%
  add_predictions(survive_ageXsex) %&gt;%
  mutate(pred = logit2prob(pred))
titanic_ageXsex</code></pre>
<pre><code># A tibble: 178 x 3
     Age Sex     pred
   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;
 1  0.42 female 0.646
 2  0.42 male   0.325
 3  0.67 female 0.647
 4  0.67 male   0.323
 5  0.75 female 0.648
 6  0.75 male   0.323
 7  0.83 female 0.648
 8  0.83 male   0.323
 9  0.92 female 0.648
10  0.92 male   0.322
# ... with 168 more rows</code></pre>
<pre class="r"><code>## So how to interpret this?
ggplot(titanic_ageXsex, aes(Age, pred, color = Sex)) +
  geom_line() +
  labs(title = &quot;Probability of Surviving the Titanic&quot;,
       y = &quot;Predicted Probability of Survival&quot;,
       color = &quot;Sex&quot;)</code></pre>
<pre><code>Warning: Removed 2 rows containing missing values (geom_path).</code></pre>
<p><img src="figure/lecture8.Rmd/unnamed-chunk-23-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>extra_model &lt;- glm(formula = Survived ~ Age + Sex + 
                     factor(Pclass) +
                     factor(Pclass):Sex, 
                   family = binomial, 
                       data = titanic_train)
summary(extra_model)</code></pre>
<pre><code>
Call:
glm(formula = Survived ~ Age + Sex + factor(Pclass) + factor(Pclass):Sex, 
    family = binomial, data = titanic_train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.1093  -0.6339  -0.4620   0.3896   2.5299  

Coefficients:
                         Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)              4.909854   0.682782   7.191 6.43e-13 ***
Age                     -0.041934   0.008184  -5.124 2.99e-07 ***
Sexmale                 -3.639225   0.628523  -5.790 7.03e-09 ***
factor(Pclass)2         -1.160437   0.734046  -1.581  0.11391    
factor(Pclass)3         -4.169187   0.648309  -6.431 1.27e-10 ***
Sexmale:factor(Pclass)2 -0.668839   0.815889  -0.820  0.41235    
Sexmale:factor(Pclass)3  2.192651   0.685268   3.200  0.00138 ** 
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 964.52  on 713  degrees of freedom
Residual deviance: 613.43  on 707  degrees of freedom
  (177 observations deleted due to missingness)
AIC: 627.43

Number of Fisher Scoring iterations: 6</code></pre>
</div>
<div id="model-selection-comparing-models" class="section level2">
<h2>Model selection / comparing models</h2>
<pre class="r"><code>##  So how do you know if a model is good or bad? How do you choose
##   the best predictors? How can we build models as robustly as possible?

## Model validation strategy:
# -- Randomly   divide  data    into: 
#   (1) Training data (~60-80%)
#   (2) Test data (remainder)
# -- Build model with training data
# -- Fit model to test data and evaluate performance
# * Categorical data: accuracy, PPV, TPR, FNR, AUC, ...
# * Numeric response: RMSE = sqrt(1/n * sum((pred - obs)^2))


model1 &lt;- lm(Sepal.Length ~ Petal.Length, data = iris)
model2 &lt;- lm(Sepal.Length ~ Petal.Length + Species, data = iris)
model3 &lt;- lm(Sepal.Length ~ Petal.Length * Species, data = iris)
model4 &lt;- lm(Sepal.Length ~ Sepal.Width + Petal.Length * Species, data = iris)
model5 &lt;- lm(Sepal.Length ~ ., data = iris)


## 1. R2 versus adjusted R2 for a linear model
##    =&gt; R2 ALWAYS increases with more predictors, beware!
## 2. What kind of prediction errors does the model make?
##    =&gt; Linear model: calculate RMSE
##    =&gt; Logistic regression: get predicted survival probabilities, convert
##       to prediction, and see how accurate you were
## 3. Backwards / forwards / stepwise regression using criteria like
##    the AIC and BIC
##    =&gt; AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion)
##    =&gt; Both account for the number of parameters to protect against overfitting
##    =&gt; AIC = 2 * (k - logLikelihood)
##    =&gt; BIC = k * log(n) - (2 * logLikelihood)
##    =&gt; where k = # of parameters, n = # of observations
##    =&gt; Prefer models with lower AICs or BICs
## 4. Likelihood ratio test (for ***nested*** models)
## 5. k-fold cross validation
## 6. ROC curves and AUC (for logistic regression)

#-------------------------------------------------------------
## In-class exercises:
##
## a) Which of the above iris models is preferred by R2 and adjusted R2?
##
## b) Get predicted Sepal.Length for model1 and model3, and calculate RMSE
## c) Get predicted survival probabilities for model with age alone and
##       model with age * sex, convert to a prediction,
##       and see what percentage of predictions were accurate.
##
## d) Perform backwards/forwards/stepwise regression 
## e) Perform a likelihood ratio test for model3 and model1 above
## f) Perform 10-fold cross validation to evaluate the best iris model.
#-------------------------------------------------------------</code></pre>
<pre class="r"><code>## 
set.seed(1234)
iris$subsample &lt;- runif(nrow(iris))
iris$test &lt;- ifelse(iris$subsample &lt; 0.90, &quot;train&quot;, &quot;test&quot;)
iris_train &lt;- filter(iris, test == &quot;train&quot;)
iris_test &lt;- filter(iris, test == &quot;test&quot;)

mod &lt;- lm(Sepal.Length ~ Sepal.Width, data = iris_train)
pred &lt;- predict(mod, iris_test)
rmse &lt;- sqrt(1/length(pred) * sum((iris_test$Sepal.Length - pred)^2))

mod2 &lt;- lm(Sepal.Length ~ Sepal.Width + Species, data = iris_train)
pred2 &lt;- predict(mod2, iris_test)
rmse2 &lt;- sqrt(1/length(pred2) * sum((iris_test$Sepal.Length - pred)^2))

## Hint for part c:
age_accuracy &lt;- titanic_train %&gt;%
  add_predictions(survive_age) %&gt;%
  mutate(pred = logit2prob(pred),
         pred = as.numeric(pred &gt; .5))
mean(age_accuracy$Survived == age_accuracy$pred, na.rm = TRUE)</code></pre>
<pre><code>[1] 0.5938375</code></pre>
<pre class="r"><code>## Hint for part d:
model &lt;- lm(Sepal.Length ~ ., data = iris)
## Backwards selection with AIC
aic.backwards &lt;- step(model, trace=TRUE) </code></pre>
<pre><code>Start:  AIC=-346.25
Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width + Species + 
    subsample + test

               Df Sum of Sq    RSS     AIC
- test          1    0.0975 13.503 -347.16
- subsample     1    0.1349 13.540 -346.75
&lt;none&gt;                      13.405 -346.25
- Petal.Width   1    0.3711 13.776 -344.15
- Species       2    0.9698 14.375 -339.77
- Sepal.Width   1    3.0427 16.448 -317.57
- Petal.Length  1   13.8350 27.240 -241.89

Step:  AIC=-347.16
Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width + Species + 
    subsample

               Df Sum of Sq    RSS     AIC
- subsample     1    0.0538 13.556 -348.57
&lt;none&gt;                      13.503 -347.16
- Petal.Width   1    0.3877 13.890 -344.92
- Species       2    0.9391 14.442 -341.08
- Sepal.Width   1    3.1092 16.612 -318.08
- Petal.Length  1   13.7486 27.251 -243.83

Step:  AIC=-348.57
Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width + Species

               Df Sum of Sq    RSS     AIC
&lt;none&gt;                      13.556 -348.57
- Petal.Width   1    0.4090 13.966 -346.11
- Species       2    0.8889 14.445 -343.04
- Sepal.Width   1    3.1250 16.681 -319.45
- Petal.Length  1   13.7853 27.342 -245.33</code></pre>
<pre class="r"><code>glance(aic.backwards)</code></pre>
<pre><code># A tibble: 1 x 11
  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC
*     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1     0.867         0.863 0.307      188. 2.67e-61     6  -32.6  79.1  100.
# ... with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;</code></pre>
<pre class="r"><code>tidy(aic.backwards)</code></pre>
<pre><code># A tibble: 6 x 5
  term              estimate std.error statistic  p.value
  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
1 (Intercept)          2.17     0.280       7.76 1.43e-12
2 Sepal.Width          0.496    0.0861      5.76 4.87e- 8
3 Petal.Length         0.829    0.0685     12.1  1.07e-23
4 Petal.Width         -0.315    0.151      -2.08 3.89e- 2
5 Speciesversicolor   -0.724    0.240      -3.01 3.06e- 3
6 Speciesvirginica    -1.02     0.334      -3.07 2.58e- 3</code></pre>
<pre class="r"><code>## Stepwise selection with BIC, for example
bic.step&lt;- step(model, trace=TRUE, criterion = &quot;BIC&quot;, direction=&quot;both&quot;)</code></pre>
<pre><code>Start:  AIC=-346.25
Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width + Species + 
    subsample + test

               Df Sum of Sq    RSS     AIC
- test          1    0.0975 13.503 -347.16
- subsample     1    0.1349 13.540 -346.75
&lt;none&gt;                      13.405 -346.25
- Petal.Width   1    0.3711 13.776 -344.15
- Species       2    0.9698 14.375 -339.77
- Sepal.Width   1    3.0427 16.448 -317.57
- Petal.Length  1   13.8350 27.240 -241.89

Step:  AIC=-347.16
Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width + Species + 
    subsample

               Df Sum of Sq    RSS     AIC
- subsample     1    0.0538 13.556 -348.57
&lt;none&gt;                      13.503 -347.16
+ test          1    0.0975 13.405 -346.25
- Petal.Width   1    0.3877 13.890 -344.92
- Species       2    0.9391 14.442 -341.08
- Sepal.Width   1    3.1092 16.612 -318.08
- Petal.Length  1   13.7486 27.251 -243.83

Step:  AIC=-348.57
Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width + Species

               Df Sum of Sq    RSS     AIC
&lt;none&gt;                      13.556 -348.57
+ subsample     1    0.0538 13.503 -347.16
+ test          1    0.0164 13.540 -346.75
- Petal.Width   1    0.4090 13.966 -346.11
- Species       2    0.8889 14.445 -343.04
- Sepal.Width   1    3.1250 16.681 -319.45
- Petal.Length  1   13.7853 27.342 -245.33</code></pre>
<pre class="r"><code>## Hint for part e:
alt_model &lt;- glance(model3)
null_model &lt;- glance(model1)
D &lt;- 2 * (alt_model$logLik - null_model$logLik) ### test statistic 
df &lt;- alt_model$df - null_model$df             ### chisquared degrees of freedom
1 - pchisq(D,df)</code></pre>
<pre><code>[1] 1.566414e-12</code></pre>
<pre class="r"><code>## ROC (=receiver operating characteristic) curves and AUC (= area under the curve)
## Let&#39;s talk about a confusion matrix
##  TPR =   TP/P    =   TP/(TP+FN) =&gt; Sensitivity
##  TNR =   TN/N    =   TN/(FP+TN) =&gt; Specificiy
##  FPR =   FP/N    =   FP/(FP+TN)  =   1   - TNR   =&gt; False positive rate
##  TP/(TP+FP) =&gt; Precision
##  FP/(FP+TP)  =   1- PPV =&gt; False discovery rate
##  (TP +   TN) /   (TP +   TN  +   FP  +   FN) =&gt; Accuracy

## Let&#39;s calculate these values for our model, assuming pred &gt; 0.5 =&gt; Survived
library(pROC)</code></pre>
<pre><code>Type &#39;citation(&quot;pROC&quot;)&#39; for a citation.</code></pre>
<pre><code>
Attaching package: &#39;pROC&#39;</code></pre>
<pre><code>The following objects are masked from &#39;package:stats&#39;:

    cov, smooth, var</code></pre>
<pre class="r"><code>## Let&#39;s use the age x sex titanic logistic regression model
## create ROC object
survive_ageXsex &lt;- glm(formula = Survived ~ Age * Sex, family = binomial, 
                       data = titanic_train)
roc.object &lt;- roc(titanic_train$Survived[!is.na(titanic_train$Age)], 
                  survive_ageXsex$linear.predictors)
roc.object$auc</code></pre>
<pre><code>Area under the curve: 0.787</code></pre>
<pre class="r"><code>## Plot ROC curves
roc.data &lt;- tibble(x = roc.object$specificities,   ### TNR (1-FPR)
                     y = roc.object$sensitivities) ### TPR
ggplot(roc.data, aes(x = x, y = y)) +
  geom_line() + scale_x_reverse() +
  ylab(&quot;Sensitivity&quot;) +
  xlab(&quot;Specificity&quot;)</code></pre>
<p><img src="figure/lecture8.Rmd/unnamed-chunk-26-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="mutiple-testing" class="section level2">
<h2>Mutiple testing</h2>
<pre class="r"><code>#=============================================================
## Quick discussion: What do you do when model diagnostics don&#39;t look right?
#=============================================================

## Model misspecification? (e.g. link, or add another regressor or higher-order
##   term)
## Nonlinear relationship between covariate and response
## (nonlinear) transformation of a covariate and/or response variable? e.g. log



#=============================================================
## Multiple testing
#=============================================================

## What&#39;s the issue?
## Consider the following: We have 1 y-variable and 100 different x-variables
y &lt;- rnorm(n=50)
x &lt;- matrix(0, ncol=100, nrow=50)
for(i in 1:100){
  x[,i] &lt;- rnorm(n=50)
}

## Suppose we test for the correlation of y with each of these 100 
## different x-variables
p.vector &lt;- NULL
for(i in 1:100){
  mod &lt;- lm(y ~ x[,i])
  p.vector[i] &lt;- summary(mod)$coef[2,4]
}

## If our criteiron for &quot;rejecting&quot; the null hypothesis is p &lt; 0.05,
## how many null hypotheses would we reject?
## Does this control the Family-wise error rate (FWER)?



## So what criterion would control the FWER at 5%?
##   =&gt; Only if p &lt; 0.05/100 (this is called the Bonferroni correction)



## Simulations to evaulate both scenarios

uncorrected &lt;- NULL
bonf.corrected &lt;- NULL

for(j in 1:1000){

  #### Simulate the data    
  y &lt;- rnorm(n=50)
  x &lt;- matrix(0, ncol=100, nrow=50)
  for(i in 1:100){
    x[,i] &lt;- rnorm(n=50)
  }
  
  #### Run 100 regressions
  p.vector &lt;- NULL
  for(i in 1:100){
    mod &lt;- lm(y ~ x[,i])
    p.vector[i] &lt;- summary(mod)$coef[2,4]
  }
  
  #### Count the number of false discoveries
  uncorrected[j] &lt;- sum(p.vector &lt; 0.05)
  bonf.corrected[j] &lt;- sum(p.vector &lt; 0.05/100)
}

mean(uncorrected)
mean(bonf.corrected)</code></pre>
<pre class="r"><code>## What is the False-Discovery Rate? How do we control the FDR?
##  =&gt; We can use the p.adjust() function.

y &lt;- rnorm(n=50)
x &lt;- matrix(0, ncol=100, nrow=50)
for(i in 1:100){
  x[,i] &lt;- rnorm(n=50)
}</code></pre>
<pre class="r"><code>## Suppose we test for the correlation of y with each of these 100 
## different x-variables
p.vector &lt;- NULL
for(i in 1:100){
  mod &lt;- lm(y ~ x[,i])
  p.vector[i] &lt;- summary(mod)$coef[2,4]
}

p.bh.adjusted &lt;- p.adjust(p.vector, method=&#39;BH&#39;)
p.bonf.adjusted &lt;- p.adjust(p.vector, method=&#39;bonferroni&#39;)

sum(p.bh.adjusted &lt; 0.05)</code></pre>
<pre><code>[1] 0</code></pre>
<pre class="r"><code>sum(p.bonf.adjusted &lt; 0.05)</code></pre>
<pre><code>[1] 0</code></pre>
<pre class="r"><code>#-------------------------------------------------------------
## In-class exercises:
##
## Write a simulation to see how well p.bh.adjusted controls FDR.
## Write a simulation to see how well p.bonf.adjusted controls FWER.
#-------------------------------------------------------------

#=============================================================
## Next time: HW 5 due. 
## Computational statistics, simulation, Monte Carlo, bootstrapping,
## multiple testing. Maybe Simpson&#39;s paradox also.
#=============================================================



logit2prob &lt;- function(x) {
  exp(x) / (1 + exp(x))
}</code></pre>
</div>
<div id="session-information" class="section level2">
<h2>Session information</h2>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>R version 3.5.1 (2018-07-02)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 17134)

Matrix products: default

locale:
[1] LC_COLLATE=English_United States.1252 
[2] LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] pROC_1.13.0     titanic_0.1.0   GGally_1.4.0    modelr_0.1.2   
 [5] bindrcpp_0.2.2  broom_0.5.0     forcats_0.3.0   stringr_1.3.1  
 [9] dplyr_0.7.7     purrr_0.2.5     readr_1.1.1     tidyr_0.8.2    
[13] tibble_1.4.2    ggplot2_3.1.0   tidyverse_1.2.1

loaded via a namespace (and not attached):
 [1] tidyselect_0.2.5   reshape2_1.4.3     haven_1.1.2       
 [4] lattice_0.20-35    colorspace_1.3-2   htmltools_0.3.6   
 [7] yaml_2.2.0         utf8_1.1.4         rlang_0.3.0.1     
[10] R.oo_1.22.0        pillar_1.3.0       glue_1.3.0        
[13] withr_2.1.2        R.utils_2.7.0      RColorBrewer_1.1-2
[16] readxl_1.1.0       bindr_0.1.1        plyr_1.8.4        
[19] munsell_0.5.0      gtable_0.2.0       workflowr_1.1.1   
[22] cellranger_1.1.0   rvest_0.3.2        R.methodsS3_1.7.1 
[25] evaluate_0.12      labeling_0.3       knitr_1.20        
[28] fansi_0.4.0        Rcpp_0.12.19       scales_1.0.0      
[31] backports_1.1.2    jsonlite_1.5       hms_0.4.2         
[34] digest_0.6.18      stringi_1.2.4      grid_3.5.1        
[37] rprojroot_1.3-2    cli_1.0.1          tools_3.5.1       
[40] magrittr_1.5       lazyeval_0.2.1     crayon_1.3.4      
[43] whisker_0.3-2      pkgconfig_2.0.2    xml2_1.2.0        
[46] lubridate_1.7.4    reshape_0.8.8      assertthat_0.2.0  
[49] rmarkdown_1.10     httr_1.3.1         rstudioapi_0.8    
[52] R6_2.3.0           nlme_3.1-137       git2r_0.23.0      
[55] compiler_3.5.1    </code></pre>
</div>

<!-- Adjust MathJax settings so that all math formulae are shown using
TeX fonts only; see
http://docs.mathjax.org/en/latest/configuration.html.  This will make
the presentation more consistent at the cost of the webpage sometimes
taking slightly longer to load. Note that this only works because the
footer is added to webpages before the MathJax javascript. -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>

<hr>
<p>
  This reproducible <a href="http://rmarkdown.rstudio.com">R Markdown</a>
  analysis was created with
  <a href="https://github.com/jdblischak/workflowr">workflowr</a> 1.1.1
</p>
<hr>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
